---
title: "CROSS VALIDATION TECHNIQUES"
output:
  pdf_document: default
  html_notebook: default
---
```{r}
setwd('C:/Users/USER/Desktop/JUPYTER_NOTEBOOK/A_MYTUTORIALS/MYR')
# saving a data as csv file
write.csv(marketing,file = 'data/marking.csv')
```
# Cross-validation 
***Cross-validation *** One of the finest techniques to check the generalization power of a machine learning model is Cross-validation techniques. Cross-validation is a resampling approach which are can be computationally expensive, because they involve fitting the same model multiple times using different subsets of the training data. **Cross-validation  ** refers to a set of methods for measuring the performance of a given predictive model.

1. by dividing the available data set into two sets namely training and testing (validation) data set. 

2. Train the model using the training data set

3. Test the effectiveness of the model on the the reserved sample (testing) of the data set and estimate the prediction error.

**cross-validation methods for assessing model performance.**
         
         Validation set approach (or data split)
         Leave One Out Cross Validation
         k-fold Cross Validation
         Repeated k-fold Cross Validation
         
## The Validation Set Approach

1. randomly dividing the available data set into two parts namely, training data set and  validation data set.
    
2. Model is trained on the training data set
    
3. The Trained model is then used to predict observations in the validation set to test the generalization ability of the model when faced with new observations by calculating the prediction error using model performance metrics





```{r}
library(tidyverse)
library(caret)
```
# Marketing Data Set
**Description**

 The impact of three advertising medias (youtube, facebook and newspaper) on sales. Data are the advertising budget in thousands of dollars along with the sales.  

```{r}
data("marketing", package = "datarium")
head(marketing,3)
```
```{r}
cat("The advertising experiment has dataset",nrow(marketing),'observations and',ncol(marketing),'features')

```


# partitioning the data set into training and testing set

```{r}
random_sample<-createDataPartition(marketing$sales,p = 0.7,list = FALSE)
training_set<-marketing[random_sample,]
testing_set<-marketing[-random_sample,]
```

Training the model
```{r}
model<-lm(sales~.,data=training_set)
```

Testing the model
```{r}
prediction<-model %>% predict(testing_set)
```


```{r}
data.frame( R2 = R2(prediction, testing_set$sales),
            RMSE = RMSE(prediction, testing_set$sales),
            MAE = MAE(prediction, testing_set$sales))
```
 Using RMSE, the prediction error rate is calculated by dividing the RMSE by the average value of the outcome variable, which should be as small as possible
 
```{r}
RMSE(prediction, testing_set$sales)/mean(testing_set$sales)
```

## NOTE
the validation set approach is only useful when a large data set is available. The model is trained on only a subset of the data set so it is possible the model will not be able to capture certain patterns or interesting information about data which are only present in the test data, leading to higher bias. The estimate of the test error rate can be highly variable, depending on precisely which observations are included in the training set and which observations are included in the validation set



# LEAVE ONE OUT CROSS VALIDATION- LOOCV

Leave out one data point and build the model on the rest of the data set. Each sample is used once as a test set  while the remaining samples form the training set. The LOOCV estimate for the test error is the average of these n test error estimates.
```{r}
loocv_data<-trainControl(method = 'LOOCV')
loocv_model<-train(sales~.,data=marketing,method='lm',trControl=loocv_data)
loocv_model
```


Although in LOOCV method, we make use all data points reducing potential bias, it is a poor estimate
because it is highly variable, since it is based upon a single observation especially if some data points are outliers and has higher execution time when n is extremely large.

# Note: 
***LeaveOneOut is equivalent to KFold(n_splits=n)***



***K-Fold Cross-Validation*** 

In practice if we have enough data, we set aside part of the data set known as the validation set and use it to measure the performance of our model prediction but since data are often scarce, this is usually not possible and the best practice in such situations is to use **K-fold cross-validation**.

**K-fold cross-validation** 

1.  Randomly split the data set into k-subsets (or k-fold) 
2. Train the model on K-1  subsets
3. Test the model on the reserved subset and record the prediction error
4. Repeat this process until each of the k subsets has served as the test set.
5. The average of the K validation scores is then obtained and used as the validation score for the model and is known as the cross-validation error .


```{r}
k_data<-trainControl(method = 'cv',number = 5)
cv_model<-train(sales~.,data=marketing,method='lm',trControl=k_data)
cv_model
```


# REPEATED K-FOLD CROSS-VALIDATION

The process of splitting the data into k-folds can be repeated a number of times, this is called repeated k-fold cross validation.

number -the number of folds 

repeats	For repeated k-fold cross-validation only: the number of complete sets of folds to compute
```{r}
#krep_data<-trainControl(method ="repeatedcv",number = 5,repeats = 3)
#k_repeat_cv<-train(sales~.,marketing , method="rf",trControl=krep_data)
train(sales~.,marketing , method="lm",trControl=trainControl(method ="repeatedcv",number = 5,repeats = 3),preProcess='scale')
```


